# Speech Recognition:

## Problem statement:

Automatic speech recognition, i.e. translating of speech into text, is still a challenging task due to the high variations in the speech signals with respect to the acsent, modulation and backgroud noises. The main objective of this project is to build an model to predict the text from the speech with the given dataset.

**Project question**: Can we build a model that can translate, with accuracy, the spoken words (in English) to text?


## Dataset: 

The dataset that will be used for this project will be derived from this site: [Google Speech Commands Dataset] (https://research.googleblog.com/2017/08/launching-speech-commands-dataset.html). 

* This dataset contains a set of one-second .wav audio files, each containing a single spoken English word. These words are from a small set of commands, collected using crowdsourcing and are spoken by a variety of different speakers. 
* Twenty core command words were recorded, with most speakers saying each of them five times. The core words are "Yes", "No", "Up", "Down", "Left", "Right", "On", "Off", "Stop", "Go", "Zero", "One", "Two", "Three", "Four", "Five", "Six", "Seven", "Eight", and "Nine". 
* To help distinguish unrecognized words, there are also ten auxiliary words, which most speakers only said once. These include "Bed", "Bird", "Cat", "Dog", "Happy", "House", "Marvin", "Sheila", "Tree", and "Wow". 
* The audio files are organized into folders based on the word they contain, and no details were recorded for any of the participants.

